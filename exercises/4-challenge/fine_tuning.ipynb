{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHScsfah94sn"
      },
      "source": [
        "# Fine Tuning Tiny Yolo\n",
        "\n",
        "The Tiny Yolo Network is fine tuned to only detect people."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULYvrymCHiBe"
      },
      "source": [
        "## Prepare Workspace"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rT5XrmzZIpk2"
      },
      "source": [
        "### Define Google Drive Flag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gxgG6hMeI2mn"
      },
      "outputs": [],
      "source": [
        "GOOGLE_DRIVE = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_yxkMvKHk8Q"
      },
      "source": [
        "### Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eoGb75c63blH"
      },
      "outputs": [],
      "source": [
        "if GOOGLE_DRIVE:\n",
        "    import os\n",
        "    from google.colab import drive\n",
        "\n",
        "    # Check if Google Drive is already mounted\n",
        "    if not os.path.exists('/content/drive/My Drive'):\n",
        "        print(\"Mounting Google Drive...\")\n",
        "        drive.mount('/content/drive')\n",
        "    else:\n",
        "        print(\"Google Drive is already mounted.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGb2-12sHuSW"
      },
      "source": [
        "### Set-up Directories & Install Libraires\n",
        "Create the directories needed and copy uploaded files into them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "oMlA8hKU93Vi"
      },
      "outputs": [],
      "source": [
        "if GOOGLE_DRIVE:\n",
        "    !pip install torchinfo\n",
        "    !pip install torchvision pillow\n",
        "\n",
        "    !mkdir /content/data\n",
        "\n",
        "    !cp /content/drive/MyDrive/eml_challenge/data/person_indices.json /content/data\n",
        "    !cp -r /content/drive/MyDrive/eml_challenge/utils /content\n",
        "    !cp /content/drive/MyDrive/eml_challenge/tinyyolov2.py /content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzIVyeeGq_Sg"
      },
      "source": [
        "### Define Path to Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "kAiKUHurq_aS"
      },
      "outputs": [],
      "source": [
        "if GOOGLE_DRIVE:\n",
        "    WEIGHTS_PATH = \"/content/drive/MyDrive/eml_challenge/weights/\"\n",
        "else:\n",
        "    WEIGHTS_PATH = \"./\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCi-d3NlWeRl"
      },
      "source": [
        "### Append Directory Paths to System Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "75a9jpWp9WfF"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "if GOOGLE_DRIVE:\n",
        "    sys.path.append('/content')\n",
        "    sys.path.append('/content/data')\n",
        "    sys.path.append('/content/utils')\n",
        "    sys.path.append(WEIGHTS_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWzCwEErXfKE"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "idmHICjM9Vq8"
      },
      "outputs": [],
      "source": [
        "# Pytorch libraries\n",
        "import torch\n",
        "import torchinfo\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Subset\n",
        "from torchvision.transforms import v2\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau # Import ReduceLROnPlateau to reduce learning rate of optimizer after Plateau\n",
        "\n",
        "# Other libraires\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tqdm\n",
        "\n",
        "# EML libraires\n",
        "from tinyyolov2 import TinyYoloV2\n",
        "from utils.loss import YoloLoss\n",
        "from utils.dataloader_v2 import VOCDataset\n",
        "from utils.ap import precision_recall_levels, ap, display_roc\n",
        "from utils.yolo import nms, filter_boxes\n",
        "from utils.viz import display_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZmM7WjDXXzw"
      },
      "source": [
        "## Define Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzzegHdlzFrq"
      },
      "source": [
        "### Define split_dataset Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "T3uAjfo_zF2L"
      },
      "outputs": [],
      "source": [
        "def split_dataset(dataset: torch.utils.data.Dataset, train_ratio: float, test_ratio: float, validation_ratio: float, pipeline=None):\n",
        "    if (train_ratio + validation_ratio + test_ratio != 1):\n",
        "        raise ValueError(\"The sum of the ratios must be equal to 1.\")\n",
        "\n",
        "    dataset_size = len(dataset)\n",
        "    indices = list(range(dataset_size))\n",
        "\n",
        "    # Step 2: Split into train+validation and test\n",
        "    train_validation_indices, test_indices = train_test_split(indices, test_size=int(test_ratio*dataset_size), random_state=42)\n",
        "    # Step 3: Split train+validation into train and val\n",
        "    train_indices, validation_indices = train_test_split(train_validation_indices, test_size=int(validation_ratio*dataset_size), random_state=42)\n",
        "\n",
        "    # Create Subsets\n",
        "    train_dataset = Subset(dataset, train_indices)\n",
        "    validation_dataset = Subset(dataset, validation_indices)\n",
        "    test_dataset = Subset(dataset, test_indices)\n",
        "\n",
        "    if pipeline is not None:\n",
        "        train_dataset.transform = pipeline\n",
        "\n",
        "    return train_dataset, validation_dataset, test_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqV106CvXmNV"
      },
      "source": [
        "### Define Early Stopping Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIaSazwOVty3"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, verbose=False, delta=0,\n",
        "                 path='/content/drive/MyDrive/eml_challenge/weights/checkpoint.pt',\n",
        "                 best_model_path='/content/drive/MyDrive/eml_challenge/weights/voc_fine_tuned.pt'):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last improvement.\n",
        "            verbose (bool): If True, prints a message for each validation metric improvement.\n",
        "            delta (float): Minimum change in the monitored metric to qualify as an improvement.\n",
        "            path (str): Path to save the best model checkpoint.\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "        self.best_model_path = best_model_path\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.avg_precision_min = 0 # Track the minimum average precision\n",
        "\n",
        "    def __call__(self, avg_precision, model):\n",
        "        score = avg_precision  # Positiv because we maximize AP\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(avg_precision, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            if self.verbose:\n",
        "                print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "                torch.save(model.state_dict(), self.best_model_path)\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(avg_precision, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, avg_precision, model):\n",
        "        \"\"\"Save model when average precision increases.\"\"\"\n",
        "        if self.verbose:\n",
        "            print(f\"Average Precision increased ({self.avg_precision_min:.6f} --> {avg_precision:.6f}). Saving model...\")\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.avg_precision_min = avg_precision\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BP9hv7w7XKv6"
      },
      "source": [
        "### Define train, validate and test Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "5wrk4ghxXK6-"
      },
      "outputs": [],
      "source": [
        "def train(net: nn.Module, data_loader: torch.utils.data.DataLoader, optimizer, criterion, device):\n",
        "    \"\"\"\n",
        "    Description:\n",
        "    This function trains the network for one epoch and returns average loss.\n",
        "\n",
        "    Args:\n",
        "    net: the network to train\n",
        "    data_loader: the data loader for the training set\n",
        "    optimizer: the optimizer to use for training\n",
        "    criterion: the loss function to use for training\n",
        "    device: the device to use for training\n",
        "    \"\"\"\n",
        "\n",
        "    total_loss = 0\n",
        "\n",
        "    net.train()\n",
        "    # Move weights to device\n",
        "    net.to(device)\n",
        "\n",
        "    for idx, (input, target) in tqdm.tqdm(enumerate(data_loader), total=len(data_loader)):\n",
        "        # Move Inputs and targets to Device\n",
        "        input  = input.to(device)\n",
        "        target = target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        #Yolo head is implemented in the loss for training, therefore yolo=False\n",
        "        output = net(input, yolo=False)\n",
        "        loss, _ = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    average_loss = total_loss / len(data_loader)\n",
        "\n",
        "    return average_loss\n",
        "\n",
        "def validate(net: nn.Module, data_loader: torch.utils.data.DataLoader, device):\n",
        "    \"\"\"\n",
        "    Description:\n",
        "    This function is used to validate the network. It is currently needed for\n",
        "    early stopping and learning rate adjustment.\n",
        "\n",
        "    Args:\n",
        "    net: the network to test\n",
        "    data_loader: the data loader for the test set\n",
        "    device: the device to use for training\n",
        "    \"\"\"\n",
        "\n",
        "    eval_precision = []\n",
        "    eval_recall = []\n",
        "\n",
        "    net.eval()\n",
        "    # Move weights to device\n",
        "    net.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, (input, target) in tqdm.tqdm(enumerate(data_loader), total=len(data_loader)):\n",
        "            input  = input.to(device)\n",
        "            target = target.to(device)\n",
        "            output = net(input, yolo=True)\n",
        "            #The right threshold values can be adjusted for the target application\n",
        "            output = filter_boxes(output, CONFIDENCE_THRESHOLD)\n",
        "            output = nms(output, NMS_THRESHOLD)\n",
        "            if idx == 0:\n",
        "                input  = input.to(torch.device('cpu'))\n",
        "                target = target.to(torch.device('cpu'))\n",
        "                input  = input.to(device)\n",
        "                target = target.to(device)\n",
        "            # Calculate precision and recall for each sample\n",
        "            for i in range(len(target)):\n",
        "                precision, recall = precision_recall_levels(target[i], output[i])\n",
        "                eval_precision.append(precision)\n",
        "                eval_recall.append(recall)\n",
        "\n",
        "    # Calculate average precision with collected samples\n",
        "    average_precision = ap(eval_precision, eval_recall)\n",
        "    # Plot ROC\n",
        "    display_roc(eval_precision, eval_recall)\n",
        "\n",
        "    return average_precision\n",
        "\n",
        "\n",
        "def test(net: nn.Module, data_loader: torch.utils.data.DataLoader, device, best_model_path):\n",
        "    \"\"\"\n",
        "    Description:\n",
        "    This function tests the network.\n",
        "\n",
        "    Args:\n",
        "    net: the network to test\n",
        "    data_loader: the data loader for the test set\n",
        "    device: the device to use for training\n",
        "    num_validation_samples: the number of passed images to the validate function\n",
        "    \"\"\"\n",
        "\n",
        "    test_precision = []\n",
        "    test_recall = []\n",
        "\n",
        "    # Load weights and move them to device\n",
        "    sd = torch.load(best_model_path, weights_only=True)\n",
        "    net.load_state_dict(sd)\n",
        "    net.to(device)\n",
        "    net.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, (input, target) in tqdm.tqdm(enumerate(data_loader), total=len(data_loader)):\n",
        "            input  = input.to(device)\n",
        "            target = target.to(device)\n",
        "            output = net(input, yolo=True)\n",
        "            #The right threshold values can be adjusted for the target application\n",
        "            output = filter_boxes(output, CONFIDENCE_THRESHOLD)\n",
        "            output = nms(output, NMS_THRESHOLD)\n",
        "            # Calculate precision and recall for each sample\n",
        "            for i in range(len(target)):\n",
        "                precision, recall = precision_recall_levels(target[i], output[i])\n",
        "                test_precision.append(precision)\n",
        "                test_recall.append(recall)\n",
        "\n",
        "    # Calculate average precision with collected samples\n",
        "    average_precision = ap(test_precision, test_recall)\n",
        "    # Plot ROC\n",
        "    display_roc(test_precision, test_recall)\n",
        "\n",
        "    return average_precision\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sVXLO9yqzQM"
      },
      "source": [
        "### Define plot_loss Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "nrnmZpZMqzZL"
      },
      "outputs": [],
      "source": [
        "def plot_loss(losses):\n",
        "    \"\"\"\n",
        "    Plots the losses over epochs.\n",
        "\n",
        "    Args:\n",
        "        losses (list of float): List of loss values, one for each epoch.\n",
        "    \"\"\"\n",
        "    epochs = range(1, len(losses) + 1)  # Epoch numbers start at 1\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(epochs, losses, marker='o', label='Loss')\n",
        "    plt.title(\"Loss over Epochs\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkMadXlHXyYT"
      },
      "source": [
        "### Define fine_tune Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rBCP5BAT9ijI"
      },
      "outputs": [],
      "source": [
        "def fine_tune(net: nn.Module,\n",
        "              sd,\n",
        "              data_loader: torch.utils.data.DataLoader,\n",
        "              validation_loader: torch.utils.data.DataLoader,\n",
        "              test_loader: torch.utils.data.DataLoader):\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "      torch_device = torch.device(\"cuda\")\n",
        "      print(\"Using GPU\")\n",
        "    else:\n",
        "      torch_device = torch.device(\"cpu\")\n",
        "      print(\"Using CPU\")\n",
        "\n",
        "    eval_AP = []\n",
        "    epoch_loss_list = []\n",
        "\n",
        "    #We load all parameters from the pretrained dict except for the last layer\n",
        "    net.load_state_dict({k: v for k, v in sd.items() if not '9' in k}, strict=False)\n",
        "\n",
        "    #We only train the last layer (conv9)\n",
        "    for key, param in net.named_parameters():\n",
        "        if any(x in key for x in ['1', '2', '3', '4', '5', '6', '7']):\n",
        "            param.requires_grad = False\n",
        "\n",
        "    # Definition of the loss\n",
        "    criterion = YoloLoss(anchors=net.anchors)\n",
        "\n",
        "    # Definition of the optimizer\n",
        "    learning_rate = 0.001\n",
        "    optimizer = torch.optim.Adam(filter(lambda x: x.requires_grad, net.parameters()), lr=learning_rate)\n",
        "\n",
        "    # Define the ReduceLROnPlateau scheduler\n",
        "    if ADJUST_LEARNING_RATE:\n",
        "        scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5)\n",
        "\n",
        "    # Initialize EarlyStopping\n",
        "    if EARLY_STOPPING:\n",
        "        early_stopping = EarlyStopping(patience=5, verbose=True, path=WEIGHTS_PATH+\"checkpoint.pt\", best_model_path=WEIGHTS_PATH+\"voc_fine_tuned.pt\")\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        print(\"****************************************************************************************************************************\")\n",
        "        print(f\"Epoch: {epoch+1}\")\n",
        "\n",
        "        # Train the network\n",
        "        average_loss = train(net, data_loader, optimizer, criterion, torch_device)\n",
        "        epoch_loss_list.append(average_loss)\n",
        "\n",
        "        # Validate the network\n",
        "        average_precision = validate(net, validation_loader, torch_device)\n",
        "        eval_AP.append(average_precision)\n",
        "        print('average precision', eval_AP[-5:])\n",
        "        print(f'average precision This Epoch: {average_precision:.3%}')\n",
        "        # Adjust learning rate in case of a Plateau of AP\n",
        "        if ADJUST_LEARNING_RATE:\n",
        "            scheduler.step(average_precision)\n",
        "            print(f\"learning rate: {scheduler.get_last_lr()}\")\n",
        "        # Stop training in case there is no further improvement of AP\n",
        "        if EARLY_STOPPING:\n",
        "            early_stopping(average_precision, net)\n",
        "            if early_stopping.early_stop:\n",
        "                print(\"Early stopping triggered. Stopping training.\")\n",
        "                break\n",
        "\n",
        "    if EARLY_STOPPING:\n",
        "        if not early_stopping.early_stop:\n",
        "            torch.save(net.state_dict(), WEIGHTS_PATH + \"voc_fine_tuned.pt\")\n",
        "            print(\"No early stopping triggered. Training completed.\")\n",
        "\n",
        "    # Test the network\n",
        "    final_average_precision = test(net, test_loader, torch_device, best_model_path=WEIGHTS_PATH+\"voc_fine_tuned.pt\")\n",
        "    print(f'best validation average precision: {max(eval_AP):.3%}')\n",
        "    print(f'test average precision:            {final_average_precision:.3%}')\n",
        "    print(\"****************************************************************************************************************************\")\n",
        "\n",
        "    # Plot the loss curve\n",
        "    plot_loss(epoch_loss_list)\n",
        "\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgMb3wcNX7WS"
      },
      "source": [
        "## Execute Workspace"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6eO4UxRzhSG"
      },
      "source": [
        "### Define data augmentation pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "nRFvt-ybzhnd"
      },
      "outputs": [],
      "source": [
        "pipeline = v2.Compose([\n",
        "    v2.RandomPhotometricDistort(p=0.5),\n",
        "    v2.RandomHorizontalFlip(p=0.5),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6A1qhgFlikJ"
      },
      "source": [
        "### Define Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "acFb6EnJlmJ7"
      },
      "outputs": [],
      "source": [
        "# Number or Epochs for fine-tuning\n",
        "NUM_EPOCHS               = 30\n",
        "# Thresholds\n",
        "CONFIDENCE_THRESHOLD     = 0.2\n",
        "NMS_THRESHOLD            = 0.6\n",
        "# Dataset split ratios\n",
        "TRAIN_DATASET_RATIO      = 0.70\n",
        "TEST_DATASET_RATIO       = 0.20\n",
        "VALIDATION_DATASET_RATIO = 0.10\n",
        "# Batch sizes\n",
        "TRAIN_BATCH_SIZE         = 128\n",
        "VALIDATION_BATCH_SIZE    = 128\n",
        "TEST_BATCH_SIZE          = 128\n",
        "# Flags\n",
        "ADJUST_LEARNING_RATE     = False\n",
        "EARLY_STOPPING           = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYg9JHS8l_bB"
      },
      "source": [
        "### Define Datasets and Data Loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXsvk6hml-vU"
      },
      "outputs": [],
      "source": [
        "dataset = VOCDataset(root=\"/content/data\", year=\"2012\", image_set='trainval', transform=None, only_person=True) # Contains 4374 pictures\n",
        "\n",
        "train_dataset, validation_dataset, test_dataset = split_dataset(dataset, train_ratio=TRAIN_DATASET_RATIO, test_ratio=TEST_DATASET_RATIO, validation_ratio=VALIDATION_DATASET_RATIO, pipeline=pipeline)\n",
        "\n",
        "print(f\"Number of training samples: {len(train_dataset)}\")\n",
        "print(f\"Number of validation samples: {len(validation_dataset)}\")\n",
        "print(f\"Number of test samples: {len(test_dataset)}\")\n",
        "\n",
        "train_loader      = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n",
        "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=VALIDATION_BATCH_SIZE, shuffle=True)\n",
        "test_loader       = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=TEST_BATCH_SIZE, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uy4iXRpr1ZF9"
      },
      "source": [
        "### Improvement Ideas\n",
        "\n",
        "\n",
        "1. try different splits\n",
        "2. try different crop methods for data augmentation\n",
        "3. print loss and plot the loss curve accross no. of epochs\n",
        "4. use display_result function to evaluate confidence and NMS thesholds\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a4e549euvT5"
      },
      "source": [
        "### Fine-tune the Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L2wA0QqK9qZ-"
      },
      "outputs": [],
      "source": [
        "# We define a tinyyolo network with only two possible classes\n",
        "net = TinyYoloV2(num_classes=1)\n",
        "\n",
        "sd = torch.load(WEIGHTS_PATH + \"voc_pretrained.pt\", weights_only=True)\n",
        "\n",
        "# Fine-tune the network\n",
        "fine_tune(net, sd, train_loader, validation_loader, test_loader)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "toc_visible": true,
      "provenance": [],
      "mount_file_id": "1mZ_RhF7lqhnHz0hRq_Vc98g-h5hCCFLo",
      "authorship_tag": "ABX9TyPOC9pupBQiHe3BwV/b12Y0"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}