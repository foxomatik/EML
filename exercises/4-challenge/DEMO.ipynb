{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b14814c4-c547-4c49-b677-2d83a2fa8845",
   "metadata": {},
   "source": [
    "# Demo Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2622b8b-2529-43aa-bf90-1343be8652c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch, open-cv, etc-\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import statistics\n",
    "\n",
    "# ONNX libraries\n",
    "import onnx\n",
    "import onnxruntime\n",
    "\n",
    "# EML libraries\n",
    "from tinyyolov2_pruned import TinyYoloV2, PrunedTinyYoloV2, FusedTinyYoloV2, PrunedFusedTinyYoloV2\n",
    "from utils.yolo import nms, filter_boxes\n",
    "from utils.viz import display_result\n",
    "from utils.camera import CameraDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6e7686-8703-4a37-ad63-58e6ba690e4b",
   "metadata": {},
   "source": [
    "# Globals and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47cf1187-3ddf-4616-a69c-c36e8600e2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    }
   ],
   "source": [
    "# Thresholds\n",
    "CONFIDENCE_THRESHOLD     = 0.5\n",
    "NMS_THRESHOLD            = 0.5\n",
    "# CUDA\n",
    "if torch.cuda.is_available():\n",
    "      torch_device = torch.device(\"cuda\")\n",
    "      print(\"Using GPU\")\n",
    "else:\n",
    "    torch_device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "# FPS\n",
    "now = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccdea34-3566-4ef9-a7ce-6e8141a021cb",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06647894-9b61-4af3-918e-ef369156ad06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722e75db-f17b-47d8-893d-a2f98ffa0930",
   "metadata": {},
   "source": [
    "## Preprocess function\n",
    "- Convert to RGB\n",
    "- Normalize to (0, ..., 1)\n",
    "- Convert to torch.Tensor + add batch-dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e6205f3-8b07-4cf2-8163-408564a87263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image, target_size=(320, 320)):\n",
    "    image = torch.from_numpy(cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0)\n",
    "    \n",
    "    return image.permute(2, 0, 1).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c6520a7c-134b-4e01-a08b-07318ba541f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_onnx(image, target_size=(320, 320)):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float16) / 255.0\n",
    "    \n",
    "    return np.expand_dims(np.transpose(image, (2, 0, 1)), axis=0)\n",
    "    # image = torch.from_numpy(cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0)\n",
    "    \n",
    "    # return image.permute(2, 0, 1).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6b2e2d-0314-46d5-8ec6-3777cd430c8b",
   "metadata": {},
   "source": [
    "## Inference function\n",
    "- perform inference of given ML-model\n",
    "- filter boxes w.r.t. confidence threshold\n",
    "- perform non-maximum-suppression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89dcc1ab-0473-4d0a-afa5-8305268c5961",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(image, net, torch_device):\n",
    "    image = image.to(torch_device, dtype=torch.float32)\n",
    "    with torch.no_grad():\n",
    "        output = net(image)\n",
    "        output = filter_boxes(output, CONFIDENCE_THRESHOLD)\n",
    "        output = nms(output, NMS_THRESHOLD)\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "022898d4-545c-44c6-a105-01f637339bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_onnx(image, onnx_session, torch_device):\n",
    "    # image = image.to(torch_device, dtype=torch.float32)\n",
    "    # with torch.no_grad():\n",
    "    #     output = net(image)\n",
    "    #     output = filter_boxes(output, CONFIDENCE_THRESHOLD)\n",
    "    #     output = nms(output, NMS_THRESHOLD)\n",
    "\n",
    "    # image = image.to(dtype=torch.float16)\n",
    "    # image = image.astype(np.float16)\n",
    "    ort_input = {onnx_session.get_inputs()[0].name: image}\n",
    "    # ort_input = {onnx_session.get_inputs()[0].name: image}\n",
    "    ort_output = onnx_session.run(None, ort_input)[0]\n",
    "    ort_output = torch.from_numpy(ort_output)\n",
    "    # ort_output = ort_output.to(torch_device)\n",
    "    ort_output = filter_boxes(ort_output, CONFIDENCE_THRESHOLD)\n",
    "    ort_output = nms(ort_output, NMS_THRESHOLD)\n",
    "\n",
    "    return ort_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91dfdf64-0ee9-46a5-bf3f-7f2c0d54e2a7",
   "metadata": {},
   "source": [
    "## Postprocess function\n",
    "- draw bboxes on image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d62aa51-9b17-445c-ac2a-55d46efcb7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(image, output):\n",
    "    img_shape = 320\n",
    "\n",
    "    if output:\n",
    "        bboxes = torch.stack(output, dim=0)\n",
    "        valid = bboxes[0, :, -1] >= 0 # filter valid bboxes\n",
    "\n",
    "        if valid.any():\n",
    "            bboxes = bboxes[:, valid]  # keep only valid bboxes\n",
    "\n",
    "            # vectorized calculation\n",
    "            x_min = (bboxes[0, :, 0] - bboxes[0, :, 2] / 2) * img_shape\n",
    "            y_min = (bboxes[0, :, 1] - bboxes[0, :, 3] / 2) * img_shape\n",
    "            x_max = x_min + bboxes[0, :, 2] * img_shape\n",
    "            y_max = y_min + bboxes[0, :, 3] * img_shape\n",
    "            conf = bboxes[0, :, 4].tolist()\n",
    "\n",
    "            for i in range(len(conf)):\n",
    "                cv2.rectangle(image, (int(x_min[i]), int(y_min[i])), (int(x_max[i]), int(y_max[i])), (0, 0, 255), 2)\n",
    "                cv2.putText(image, f\"person {conf[i]:.2f}\", (int(x_min[i]), int(y_min[i]) - 5), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)\n",
    "                \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9f564100-2960-43ee-b4ff-78f2a4524119",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_onnx(image, output):\n",
    "    img_shape = 320\n",
    "\n",
    "    if output:\n",
    "        bboxes = torch.stack(output, dim=0)\n",
    "        valid = bboxes[0, :, -1] >= 0 # filter valid bboxes\n",
    "\n",
    "        if valid.any():\n",
    "            bboxes = bboxes[:, valid]  # keep only valid bboxes\n",
    "\n",
    "            # vectorized calculation\n",
    "            x_min = (bboxes[0, :, 0] - bboxes[0, :, 2] / 2) * img_shape\n",
    "            y_min = (bboxes[0, :, 1] - bboxes[0, :, 3] / 2) * img_shape\n",
    "            x_max = x_min + bboxes[0, :, 2] * img_shape\n",
    "            y_max = y_min + bboxes[0, :, 3] * img_shape\n",
    "            conf = bboxes[0, :, 4].tolist()\n",
    "\n",
    "            for i in range(len(conf)):\n",
    "                cv2.rectangle(image, (int(x_min[i]), int(y_min[i])), (int(x_max[i]), int(y_max[i])), (0, 0, 255), 2)\n",
    "                cv2.putText(image, f\"person {conf[i]:.2f}\", (int(x_min[i]), int(y_min[i]) - 5), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)\n",
    "                \n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85603c48-9d70-478a-9722-b56a2a517293",
   "metadata": {},
   "source": [
    "## Camera Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fb47006-35e3-4727-bffa-3747c0cc2f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def callback(image):\n",
    "    global now\n",
    "\n",
    "    elapsed_time = time.time() - now\n",
    "    fps = f\"{int(1 / elapsed_time) if elapsed_time > 0 else 0}\"\n",
    "    if int(fps) > 0:\n",
    "        fps_list.append(int(fps))\n",
    "    if len(fps_list) >= 500:\n",
    "        cam.stop()\n",
    "        cam.release()\n",
    "    now = time.time()\n",
    "    \n",
    "    image = image[:320, :320]\n",
    "    image_pre = preprocess(image)\n",
    "    output = inference(image_pre, model, torch_device)\n",
    "    image_post = postprocess(image, output)\n",
    "    \n",
    "    cv2.putText(image_post, f\"fps={fps}\", (2, 25), cv2.FONT_HERSHEY_SIMPLEX, 1, (100, 255, 0), 2, cv2.LINE_AA)\n",
    "    \n",
    "    return image_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "737edf09-0e46-4cb3-b2fa-24f12a3be268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def callback_onnx(image):\n",
    "    global now\n",
    "\n",
    "    elapsed_time = time.time() - now\n",
    "    fps = f\"{int(1 / elapsed_time) if elapsed_time > 0 else 0}\"\n",
    "    # if int(fps) > 0:\n",
    "    #     fps_list.append(int(fps))\n",
    "    # if len(fps_list) >= 500:\n",
    "    #     cam.stop()\n",
    "    #     cam.release()\n",
    "    now = time.time()\n",
    "    \n",
    "    image = image[:320, :320]\n",
    "    image_pre = preprocess_onnx(image)\n",
    "    output = inference_onnx(image_pre, onnx_session, torch_device)\n",
    "    image_post = postprocess_onnx(image, output)\n",
    "    \n",
    "    cv2.putText(image_post, f\"fps={fps}\", (2, 25), cv2.FONT_HERSHEY_SIMPLEX, 1, (100, 255, 0), 2, cv2.LINE_AA)\n",
    "    \n",
    "    return image_post"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806e1cf8-5fb5-4368-ace2-40f2608adfe6",
   "metadata": {},
   "source": [
    "# Camera Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7e6e70-6c9f-4742-bfca-17ad81456bce",
   "metadata": {},
   "source": [
    "## Model PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "61979d70-ef76-4a7c-b4ec-192f9d386291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GST_ARGUS: Cleaning up\n",
      "CONSUMER: Done Success\n",
      "GST_ARGUS: Done Success\n",
      "Camera released\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PrunedFusedTinyYoloV2(\n",
       "  (pad): ReflectionPad2d((0, 1, 0, 1))\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3): Conv2d(32, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4): Conv2d(20, 46, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv5): Conv2d(46, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv6): Conv2d(96, 199, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv7): Conv2d(199, 401, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv8): Conv2d(401, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv9): Conv2d(1024, 30, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd_str = f\"./weights/voc/pruning_3-7_fused/voc_fine_tuned_fused{0.9}.pt\"\n",
    "# sd_str = f\"./weights/voc/pruning_1-7/voc_fine_tuned_pruned{0.65}.pt\"\n",
    "# sd_str = f\"./weights/coco/voc_coco_fine_tuned_augmented.pt\"\n",
    "# sd_str = f\"./voc_fine_tuned.pt\"\n",
    "sd = torch.load(sd_str, weights_only=True, map_location=torch_device)\n",
    "\n",
    "model = PrunedFusedTinyYoloV2(num_classes=1)\n",
    "# model = PrunedTinyYoloV2(num_classes=1)\n",
    "# model = TinyYoloV2(num_classes=1)\n",
    "model.load_state_dict(sd, strict=False)\n",
    "# model.to(torch_device, dtype=torch.float16)\n",
    "model.to(torch_device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bf2d077f-8f38-4642-94aa-17be273e6170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing camera...\n",
      "GST_ARGUS: Cleaning up\n",
      "CONSUMER: Done Success\n",
      "GST_ARGUS: Done Success\n",
      "GST_ARGUS: Creating output stream\n",
      "CONSUMER: Waiting until producer is connected...\n",
      "GST_ARGUS: Available Sensor modes :\n",
      "GST_ARGUS: 3264 x 2464 FR = 21.000000 fps Duration = 47619048 ; Analog Gain range min 1.000000, max 10.625000; Exposure Range min 13000, max 683709000;\n",
      "\n",
      "GST_ARGUS: 3264 x 1848 FR = 28.000001 fps Duration = 35714284 ; Analog Gain range min 1.000000, max 10.625000; Exposure Range min 13000, max 683709000;\n",
      "\n",
      "GST_ARGUS: 1920 x 1080 FR = 29.999999 fps Duration = 33333334 ; Analog Gain range min 1.000000, max 10.625000; Exposure Range min 13000, max 683709000;\n",
      "\n",
      "GST_ARGUS: 1640 x 1232 FR = 29.999999 fps Duration = 33333334 ; Analog Gain range min 1.000000, max 10.625000; Exposure Range min 13000, max 683709000;\n",
      "\n",
      "GST_ARGUS: 1280 x 720 FR = 59.999999 fps Duration = 16666667 ; Analog Gain range min 1.000000, max 10.625000; Exposure Range min 13000, max 683709000;\n",
      "\n",
      "GST_ARGUS: 1280 x 720 FR = 120.000005 fps Duration = 8333333 ; Analog Gain range min 1.000000, max 10.625000; Exposure Range min 13000, max 683709000;\n",
      "\n",
      "GST_ARGUS: Running with following settings:\n",
      "   Camera index = 0 \n",
      "   Camera mode  = 5 \n",
      "   Output Stream W = 1280 H = 720 \n",
      "   seconds to Run    = 0 \n",
      "   Frame Rate = 120.000005 \n",
      "GST_ARGUS: Setup Complete, Starting captures for 0 seconds\n",
      "GST_ARGUS: Starting repeat capture requests.\n",
      "CONSUMER: Producer has connected; continuing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@2337.074] global cap_gstreamer.cpp:1777 open OpenCV | GStreamer warning: Cannot query video position: status=0, value=-1, duration=-1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0ac886ee7004e409abb327d73fd1181",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00\\xff\\xdb\\x00C\\x00\\x02\\x01\\x0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the camera with the callback\n",
    "cam = CameraDisplay(callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "844d4c4f-bf74-468c-8eef-13f3d02b3046",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-22:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jetson/embedded-ml-lab-students-ws2425/exercises/4-challenge/utils/camera.py\", line 67, in _capture_frames\n",
      "    self.value = self._read()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/traitlets/traitlets.py\", line 716, in __set__\n",
      "    self.set(obj, value)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/traitlets/traitlets.py\", line 706, in set\n",
      "    obj._notify_trait(self.name, old_value, new_value)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/traitlets/traitlets.py\", line 1513, in _notify_trait\n",
      "    self.notify_change(\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/traitlets/traitlets.py\", line 1525, in notify_change\n",
      "    return self._notify_observers(change)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/traitlets/traitlets.py\", line 1568, in _notify_observers\n",
      "    c(event)\n",
      "  File \"/home/jetson/embedded-ml-lab-students-ws2425/exercises/4-challenge/utils/camera.py\", line 115, in _camera_callback\n",
      "    image = self.img_to_display_img_callback(image)\n",
      "  File \"/tmp/ipykernel_1343/3559388654.py\", line 9, in callback\n",
      "  File \"/home/jetson/embedded-ml-lab-students-ws2425/exercises/4-challenge/utils/camera.py\", line 126, in stop\n",
      "    self.camera.running = False\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/traitlets/traitlets.py\", line 716, in __set__\n",
      "    self.set(obj, value)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/traitlets/traitlets.py\", line 706, in set\n",
      "    obj._notify_trait(self.name, old_value, new_value)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/traitlets/traitlets.py\", line 1513, in _notify_trait\n",
      "    self.notify_change(\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/traitlets/traitlets.py\", line 1525, in notify_change\n",
      "    return self._notify_observers(change)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/traitlets/traitlets.py\", line 1568, in _notify_observers\n",
      "    c(event)\n",
      "  File \"/home/jetson/embedded-ml-lab-students-ws2425/exercises/4-challenge/utils/camera.py\", line 79, in _on_running\n",
      "    self.thread.join()\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 1008, in join\n",
      "    raise RuntimeError(\"cannot join current thread\")\n",
      "RuntimeError: cannot join current thread\n"
     ]
    }
   ],
   "source": [
    "# The camera stream can be started with cam.start()\n",
    "# The callback gets asynchronously called (can be stopped with cam.stop())\n",
    "fps_list = []\n",
    "cam.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "74f78173-6527-4159-9685-0d651070530c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera released\n"
     ]
    }
   ],
   "source": [
    "# The camera should always be stopped and released for a new camera is instantiated (calling CameraDisplay(callback) again)\n",
    "# print(len(fps_list))\n",
    "# print(fps_list)\n",
    "# print(statistics.mean(fps_list))\n",
    "# print(statistics.variance(fps_list))\n",
    "\n",
    "cam.stop()\n",
    "cam.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3451e6ac-056c-45ec-85f2-e581aa75faa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057aaf11-e1b1-4a3c-b171-a5d233d135fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e188f315-16c6-4ec5-9802-438f3433bb86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "203210b9-c00f-4d9b-bbe8-ce6e0232d41a",
   "metadata": {},
   "source": [
    "## Model ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "262a3f4d-833a-4c3c-b652-673cbcaa8210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sd_str = f\"./weights/voc/pruning_3-7_fused/voc_fine_tuned_fused{0.9}.pt\"\n",
    "# sd_str = f\"./weights/voc/pruning_1-7/voc_fine_tuned_pruned{0.65}.pt\"\n",
    "# sd_str = f\"./weights/coco/voc_coco_fine_tuned_augmented.pt\"\n",
    "# sd_str = f\"./weights/voc/voc_fine_tuned.pt\"\n",
    "# sd = torch.load(sd_str, weights_only=True)\n",
    "\n",
    "# model = PrunedFusedTinyYoloV2(num_classes=1)\n",
    "# model = PrunedTinyYoloV2(num_classes=1)\n",
    "# model = TinyYoloV2(num_classes=1)\n",
    "# model.load_state_dict(sd)\n",
    "\n",
    "onnx_filepath = f\"./weights/voc/onnx_3-7/tiny_yolo_pruned_0.9.onnx\"\n",
    "provider     = [\"CUDAExecutionProvider\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "54d47bbd-8d93-4ed8-b6c9-39dab860fd60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GST_ARGUS: Cleaning up\n",
      "CONSUMER: Done Success\n",
      "GST_ARGUS: Done Success\n"
     ]
    }
   ],
   "source": [
    "# Define Inference Session Options\n",
    "session_options = onnxruntime.SessionOptions()\n",
    "session_options.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "\n",
    "# Define Inference Session for 32 Bit Float\n",
    "onnx_session = onnxruntime.InferenceSession(onnx_filepath, providers=provider, sess_options=session_options)\n",
    "# Define Inference Session for 16 Bit Float\n",
    "# ort_session_fp16 = onnxruntime.InferenceSession(onnx_filepath_fp16, providers=provider, sess_options=session_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9eb65b38-b469-4a52-9348-367b11f70b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing camera...\n",
      "GST_ARGUS: Creating output stream\n",
      "CONSUMER: Waiting until producer is connected...\n",
      "GST_ARGUS: Available Sensor modes :\n",
      "GST_ARGUS: 3264 x 2464 FR = 21.000000 fps Duration = 47619048 ; Analog Gain range min 1.000000, max 10.625000; Exposure Range min 13000, max 683709000;\n",
      "\n",
      "GST_ARGUS: 3264 x 1848 FR = 28.000001 fps Duration = 35714284 ; Analog Gain range min 1.000000, max 10.625000; Exposure Range min 13000, max 683709000;\n",
      "\n",
      "GST_ARGUS: 1920 x 1080 FR = 29.999999 fps Duration = 33333334 ; Analog Gain range min 1.000000, max 10.625000; Exposure Range min 13000, max 683709000;\n",
      "\n",
      "GST_ARGUS: 1640 x 1232 FR = 29.999999 fps Duration = 33333334 ; Analog Gain range min 1.000000, max 10.625000; Exposure Range min 13000, max 683709000;\n",
      "\n",
      "GST_ARGUS: 1280 x 720 FR = 59.999999 fps Duration = 16666667 ; Analog Gain range min 1.000000, max 10.625000; Exposure Range min 13000, max 683709000;\n",
      "\n",
      "GST_ARGUS: 1280 x 720 FR = 120.000005 fps Duration = 8333333 ; Analog Gain range min 1.000000, max 10.625000; Exposure Range min 13000, max 683709000;\n",
      "\n",
      "GST_ARGUS: Running with following settings:\n",
      "   Camera index = 0 \n",
      "   Camera mode  = 5 \n",
      "   Output Stream W = 1280 H = 720 \n",
      "   seconds to Run    = 0 \n",
      "   Frame Rate = 120.000005 \n",
      "GST_ARGUS: Setup Complete, Starting captures for 0 seconds\n",
      "GST_ARGUS: Starting repeat capture requests.\n",
      "CONSUMER: Producer has connected; continuing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@4087.854] global cap_gstreamer.cpp:1777 open OpenCV | GStreamer warning: Cannot query video position: status=0, value=-1, duration=-1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86fbef5d16244d468083785b019497f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00\\xff\\xdb\\x00C\\x00\\x02\\x01\\x0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the camera with the callback\n",
    "cam = CameraDisplay(callback_onnx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9c4f840c-2f21-4672-a262-b3c74637dc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The camera stream can be started with cam.start()\n",
    "# The callback gets asynchronously called (can be stopped with cam.stop())\n",
    "cam.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "5841feb5-20af-43ee-ad6a-30478d72539a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera released\n"
     ]
    }
   ],
   "source": [
    "# The camera should always be stopped and released for a new camera is instantiated (calling CameraDisplay(callback) again)\n",
    "cam.stop()\n",
    "cam.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8562fdd-4eee-419b-b0f5-42fa5922146f",
   "metadata": {},
   "source": [
    "# Misc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0664fb89-6491-4afd-9d2d-350cde4b40d0",
   "metadata": {},
   "source": [
    "## Old Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9416ac8b-92c7-4404-9cbf-67cf2c6ddca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image, target_size=(320, 320)):\n",
    "    # Convert from BGR (cv2) to RGB\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    # Normalisieren (z.B. Wertebereich 0-1)\n",
    "    image = image.astype(np.float32) / 255.0\n",
    "    # Convert to tensor, HWC to CHW and add batch-dimesnion\n",
    "    image = torch.from_numpy(image).permute(2, 0, 1).unsqueeze(0)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16e97d9-39d1-47cc-abd2-653fa6bac256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(image, net, torch_device):\n",
    "    image = image.to(torch_device)\n",
    "    with torch.no_grad():\n",
    "        output = net(image.float())\n",
    "        # output = net(image)\n",
    "        output = filter_boxes(output, CONFIDENCE_THRESHOLD)\n",
    "        output = nms(output, NMS_THRESHOLD)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38668b4e-e6dd-40b0-a975-b715c4db40fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(image, output):\n",
    "    img_shape = 320\n",
    "\n",
    "    if output:\n",
    "        bboxes = torch.stack(output, dim=0)\n",
    "        valid = bboxes[0, :, -1] >= 0 # filter valid bboxes\n",
    "    \n",
    "        for i in range(bboxes.shape[1]):\n",
    "            if bboxes[0,i,-1] >= 0:\n",
    "                x_min = int(bboxes[0,i,0]*img_shape - bboxes[0,i,2]*img_shape/2)\n",
    "                y_min = int(bboxes[0,i,1]*img_shape - bboxes[0,i,3]*img_shape/2)\n",
    "                x_max = x_min + int(bboxes[0,i,2]*img_shape)\n",
    "                y_max = y_min + int(bboxes[0,i,3]*img_shape)\n",
    "                conf  = float(bboxes[0,i,4])\n",
    "      \n",
    "                cv2.rectangle(image, (x_min, y_min), (x_max, y_max), (0,0,255), 2)\n",
    "                cv2.putText(image, f\"person {conf:.2f}\", (x_min, y_min-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255), 1)\n",
    "        \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dde6ed0-35d0-4a81-83ce-732813a17446",
   "metadata": {},
   "outputs": [],
   "source": [
    "def callback(image):\n",
    "    global now\n",
    "    \n",
    "    fps = f\"{int(1/(time.time() - now))}\"\n",
    "    now = time.time()\n",
    "    \n",
    "    # Crop und resize\n",
    "    # image = image[0:target_size[0], 0:target_size[1], :]\n",
    "    image = image[0:320,0:320, :]\n",
    "    \n",
    "    image_pre = preprocess(image)\n",
    "    output = inference(image_pre, model, torch_device)\n",
    "    image_post = postprocess(image, output)\n",
    "    \n",
    "    cv2.putText(image_post, \"fps=\"+fps, (2, 25), cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
    "               (100, 255, 0), 2, cv2.LINE_AA)\n",
    "    \n",
    "    return image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
