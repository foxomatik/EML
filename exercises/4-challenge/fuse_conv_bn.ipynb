{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kNEBkwrF-H7s"
   },
   "source": [
    "# Fusing Batch Norm Layers\n",
    "In this notebook the weights of the batch normalization layers in the **TinyYolo** Network will be fused with their corresponding weights of the convulational layer, in order to speed up inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zfAjcNrw-plT"
   },
   "source": [
    "## Prepare Workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OUK3knth_SWT"
   },
   "source": [
    "### Define Google Drive Flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-o-DLHTb_Yo5"
   },
   "outputs": [],
   "source": [
    "GOOGLE_DRIVE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vveDAJxe_bMM"
   },
   "source": [
    "### Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sawsH4MF_d_3"
   },
   "outputs": [],
   "source": [
    "if GOOGLE_DRIVE:\n",
    "    import os\n",
    "    from google.colab import drive\n",
    "\n",
    "    # Check if Google Drive is already mounted\n",
    "    if not os.path.exists('/content/drive/My Drive'):\n",
    "        print(\"Mounting Google Drive...\")\n",
    "        drive.mount('/content/drive')\n",
    "    else:\n",
    "        print(\"Google Drive is already mounted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VS1lmwIh_pI4"
   },
   "source": [
    "### Set-up Directories & Install Libraires\n",
    "Create the directories needed and copy uploaded files into them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7xdotbYH_myn"
   },
   "outputs": [],
   "source": [
    "if GOOGLE_DRIVE:\n",
    "    !pip install torchinfo\n",
    "    !pip install torchvision pillow\n",
    "\n",
    "    !mkdir /content/data\n",
    "\n",
    "    !cp /content/drive/MyDrive/eml_challenge/data/person_indices.json /content/data\n",
    "    !cp -r /content/drive/MyDrive/eml_challenge/utils /content\n",
    "    !cp /content/drive/MyDrive/eml_challenge/tinyyolov2.py /content\n",
    "    !cp /content/drive/MyDrive/eml_challenge/tinyyolov2_fused.py /content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9aoF0Lr8ACb3"
   },
   "source": [
    "### Define Path to Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ifz_59phAHYv"
   },
   "outputs": [],
   "source": [
    "if GOOGLE_DRIVE:\n",
    "    WEIGHTS_PATH = \"/content/drive/MyDrive/eml_challenge/weights/\"\n",
    "else:\n",
    "    WEIGHTS_PATH = \"./\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EebwBDhdAKpX"
   },
   "source": [
    "### Append Directory Paths to System Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6akk56nKCV53"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "if GOOGLE_DRIVE:\n",
    "    sys.path.append('/content')\n",
    "    sys.path.append('/content/data')\n",
    "    sys.path.append('/content/utils')\n",
    "    sys.path.append(WEIGHTS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mHI5PA1zCYsi"
   },
   "source": [
    "### Import Libraires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AfDLi48cCdqv"
   },
   "outputs": [],
   "source": [
    "# Pytorch libraries\n",
    "import torch\n",
    "import torchinfo\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# Other libraires\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import time\n",
    "\n",
    "# EML libraires\n",
    "from tinyyolov2 import TinyYoloV2\n",
    "from tinyyolov2_fused import FusedTinyYoloV2\n",
    "from utils.dataloader_v2 import VOCDataset\n",
    "from utils.ap import precision_recall_levels, ap, display_roc\n",
    "from utils.yolo import nms, filter_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vFbfDlAs_BPT"
   },
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M3I6tHrSKcVN"
   },
   "source": [
    "### Define fuse_conv_bn_weights Function\n",
    "This fucntion calculates fuses one convolutional layer with its corresponding batch normalization layer and calculates the new weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rITYflpOLeja"
   },
   "outputs": [],
   "source": [
    "def fuse_conv_bn_weights(conv_w, bn_rm, bn_rv, bn_w, bn_b):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        conv_w: shape=(output_channels, in_channels, kernel_size, kernel_size)\n",
    "        conv_b: shape=(output_channels)\n",
    "        bn_rm:  shape=(output_channels)\n",
    "        bn_rv:  shape=(output_channels)\n",
    "        bn_w:   shape=(output_channels)\n",
    "        bn_b:   shape=(output_channels)\n",
    "\n",
    "    Output:\n",
    "        fused_conv_w = shape=conv_w\n",
    "        fused_conv_b = shape=conv_b\n",
    "    \"\"\"\n",
    "    bn_eps = 1e-05\n",
    "\n",
    "    fused_conv = torch.zeros(conv_w.shape)\n",
    "    fused_bias = torch.zeros(conv_w.shape[0])\n",
    "\n",
    "    conv_b = torch.zeros(conv_w.shape[0])\n",
    "\n",
    "    for output_channel in range(conv_w.shape[0]):\n",
    "        fused_conv[output_channel] = conv_w[output_channel] * bn_w[output_channel] / ((bn_rv[output_channel] + bn_eps)** (1/2))\n",
    "        fused_bias[output_channel] = bn_b[output_channel] + ((bn_w[output_channel] * (conv_b[output_channel] - bn_rm[output_channel])) / ((bn_rv[output_channel] + bn_eps)** (1/2)))\n",
    "\n",
    "    return fused_conv, fused_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J1cqTK9aTAZo"
   },
   "source": [
    "### Define fuse_conv_bn_net Function\n",
    "This function accepts the state dictionary as well as the number of convolutional layers to be fused with their corresponding batch normalization layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z1suRvp7VsaM"
   },
   "outputs": [],
   "source": [
    "def fuse_conv_bn_net(state_dict, num_conv_layers):\n",
    "    fused_dict = {}\n",
    "\n",
    "    for i in range(1, num_conv_layers+1):\n",
    "        fused_dict[\"conv\" + str(i) + \".weight\"], fused_dict[\"conv\" + str(i) + \".bias\"] = fuse_conv_bn_weights(\n",
    "                                                                state_dict[\"conv\" + str(i) + \".weight\"],\n",
    "                                                                state_dict[\"bn\" + str(i) + \".running_mean\"],\n",
    "                                                                state_dict[\"bn\" + str(i) + \".running_var\"],\n",
    "                                                                state_dict[\"bn\" + str(i) + \".weight\"],\n",
    "                                                                state_dict[\"bn\" + str(i) + \".bias\"])\n",
    "    fused_dict[\"conv9.weight\"] = state_dict[\"conv9.weight\"]\n",
    "    fused_dict[\"conv9.bias\"]   = state_dict[\"conv9.bias\"]\n",
    "    fused_dict[\"anchors\"]      = state_dict[\"anchors\"]\n",
    "    return fused_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CUjpF5Qva0Tj"
   },
   "source": [
    "### Define net_time Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-_IBJUu-a4Ru"
   },
   "outputs": [],
   "source": [
    "def net_time(net, test_loader):\n",
    "    net.eval()\n",
    "\n",
    "    t_start = time.time()\n",
    "\n",
    "    for _, (inputs, _) in tqdm.tqdm(enumerate(test_loader), total=len(test_loader)):\n",
    "        net.forward(inputs)\n",
    "\n",
    "    t_end = time.time()\n",
    "    t = t_end - t_start\n",
    "\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jzAWOCFBjRO9"
   },
   "source": [
    "### Define test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "41rhKlcujTn1"
   },
   "outputs": [],
   "source": [
    "def test(net: nn.Module, data_loader: torch.utils.data.DataLoader, device):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    This function tests the network.\n",
    "\n",
    "    Args:\n",
    "    net: the network to test\n",
    "    data_loader: the data loader for the test set\n",
    "    device: the device to use for training\n",
    "    num_validation_samples: the number of passed images to the validate function\n",
    "    \"\"\"\n",
    "\n",
    "    test_precision = []\n",
    "    test_recall = []\n",
    "\n",
    "    net.to(device)\n",
    "    net.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (input, target) in tqdm.tqdm(enumerate(data_loader), total=len(data_loader)):\n",
    "            input  = input.to(device)\n",
    "            target = target.to(device)\n",
    "            output = net(input, yolo=True)\n",
    "            #The right threshold values can be adjusted for the target application\n",
    "            output = filter_boxes(output, CONFIDENCE_THRESHOLD)\n",
    "            output = nms(output, NMS_THRESHOLD)\n",
    "            # Calculate precision and recall for each sample\n",
    "            for i in range(len(target)):\n",
    "                precision, recall = precision_recall_levels(target[i], output[i])\n",
    "                test_precision.append(precision)\n",
    "                test_recall.append(recall)\n",
    "\n",
    "    # Calculate average precision with collected samples\n",
    "    average_precision = ap(test_precision, test_recall)\n",
    "    # Plot ROC\n",
    "    display_roc(test_precision, test_recall)\n",
    "\n",
    "    return average_precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zlWGTA1n_JbN"
   },
   "source": [
    "## Execute Workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JerxJsxUaG0M"
   },
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CNlV6AgwaIoT"
   },
   "outputs": [],
   "source": [
    "# Thresholds\n",
    "CONFIDENCE_THRESHOLD     = 0.0\n",
    "NMS_THRESHOLD            = 0.5\n",
    "# Batch Size\n",
    "TEST_BATCH_SIZE         = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SZR5sR9_gPhG"
   },
   "source": [
    "### Define Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uaau9noVgMl3",
    "outputId": "45e76809-d1a4-4632-c713-9998cfa19829"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "      torch_device = torch.device(\"cuda\")\n",
    "      print(\"Using GPU\")\n",
    "else:\n",
    "    torch_device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "net       = TinyYoloV2(num_classes=1)\n",
    "fused_net = FusedTinyYoloV2(num_classes=1)\n",
    "\n",
    "test_dataset  = VOCDataset(root=\"/content/data\", year=\"2012\", image_set='val', only_person=True)   # Contains 2232 pictures\n",
    "test_loader   = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=TEST_BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X6WmPMOzZOU7"
   },
   "source": [
    "### Fuse Batch and Conv Layers\n",
    "Fuse batch normalization layers to their corresponding convolutional layers and load them to the chosen device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ycxzBTHnGjiM",
    "outputId": "73e8d47b-a080-4097-aef0-c4e289f18d5f"
   },
   "outputs": [],
   "source": [
    "# Since the last convolutional layer doesn't have a batch normalization layer\n",
    "# after it, NUM_CONV_LAYERS was set to 8\n",
    "NUM_CONV_LAYERS = 8\n",
    "\n",
    "sd = torch.load(WEIGHTS_PATH + \"voc_fine_tuned.pt\", weights_only=True, map_location=torch_device)\n",
    "net.load_state_dict(sd, strict=False)\n",
    "\n",
    "sd_fused = fuse_conv_bn_net(state_dict=sd, num_conv_layers=NUM_CONV_LAYERS)\n",
    "fused_net.load_state_dict(sd_fused, strict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jKXG80YsZVb1"
   },
   "source": [
    "### Timing Benchmark for Inference before Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6mzr_qbdgFpY",
    "outputId": "c3887530-1ffa-4df0-af5f-df17d5d8f8b0"
   },
   "outputs": [],
   "source": [
    "print(f\"\\nInference Time before Fusion: {net_time(net, test_loader):.3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S7x-EYkrZeAR"
   },
   "source": [
    "### Timing Benchmark for Inference after Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9TekPmCilzF",
    "outputId": "22b913cf-a699-4ab9-b276-4f644790276a"
   },
   "outputs": [],
   "source": [
    "print(f\"\\nInference Time after Fusion: {net_time(fused_net, test_loader):.3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4pBx0HT7c9ZC"
   },
   "source": [
    "### Average Precision Benchmark for TinyYolov2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "R6yM9DtKi5lN",
    "outputId": "49f9d052-efd3-4dea-f3e3-e8b1270b410f"
   },
   "outputs": [],
   "source": [
    "average_precision_before_fusion = test(net, test_loader, torch_device)\n",
    "print(f\"Average Precision before Fusion: {average_precision_before_fusion:.3%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "erATCMxuc9j9"
   },
   "source": [
    "### Average Precision Benchmark for FusedTinyYolov2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "y2S3rI_-i6cs",
    "outputId": "fbdfcac7-d8ce-49be-d7bc-12895300e7b4"
   },
   "outputs": [],
   "source": [
    "average_precision_after_fusion = test(fused_net, test_loader, torch_device)\n",
    "print(f\"Average Precision after Fusion: {average_precision_after_fusion:.3%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save fused Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(fused_net.state_dict(), WEIGHTS_PATH + \"fused_voc_fine_tuned.pt\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
