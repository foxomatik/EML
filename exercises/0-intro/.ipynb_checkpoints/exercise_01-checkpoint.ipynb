{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8909361-4d07-4331-a587-be85e32a3823",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Embedded ML Lab - Excercise 0 - Intro Inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2265b134-4819-4b6a-902e-9562836b055d",
   "metadata": {
    "tags": []
   },
   "source": [
    "We start with a NN model similar to the LeNet model from 1989 (https://en.wikipedia.org/wiki/LeNet). The LeNet Model is designed to detect handwritten numbers from the MNIST dataset http://yann.lecun.com/exdb/mnist/with size 28x28 and outputs a vector with size 10, where each number in this vector represents the likelihood that the input corresponds to that number. All Conv layers have `stride=1` `padding=0`.\n",
    "\n",
    "<img src=\"src/lenet.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "<span style=\"color:green\">Your Tasks:</span>\n",
    "* <span style=\"color:green\">Write the init code for the required modules to define LeNet  (Use the provided image to determine the number of input/ouput filters and kernel sizes)</span>\n",
    "    * <span style=\"color:green\">Determine the output size of conv2 to determine the input size of fc1</span>\n",
    "    * The size of the output conv2d layer can be determined with the following formula $H_{\\text{out}} = \\lfloor{ \\frac{H_{\\text{in}} + 2 \\times \\text{padding} - 1 \\times ( \\text{kernelsize} -1 ) -1 } {\\text{stride}} +1}\\rfloor$\n",
    "    * Here, maxpool2d with kernel size 2 reduces the input size by factor two: $H_{\\text{out}} = \\lfloor \\frac{H_{\\text{in}}}{2}\\rfloor$\n",
    "    * <span style=\"color:green\">Use following modules: `nn.Conv2d, nn.Linear`</span>\n",
    "* <span style=\"color:green\">Define the forward pass of LeNet, check the provided image for the flow of data through the modules and functions</span>\n",
    "    * <span style=\"color:green\">Use the following functions: `F.relu, F.max_pool2d, tensor.flatten`</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "guided-recognition",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.rand(1).to('cuda') #initialize cuda context (might take a while)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "34cea594-90eb-4a07-b390-b3f332e7869c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        \n",
    "        #---to-be-done-by-student---\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)  # out: 26x26x6 (maxpool)-> 13x13x6\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3) # out: 11x11x16(maxpool) -> 5x5x16\n",
    "        self.fc1 = nn.Linear(400 , 120)  # in:  400x1 (prev: flatten)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "        self.relu = F.relu\n",
    "        self.maxpool = F.max_pool2d\n",
    "        \n",
    "        #---end---------------------\n",
    "        return\n",
    "    \n",
    "    def forward(self,x):\n",
    "        #---to-be-done-by-student---\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool(self.relu(x), 2)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.maxpool(self.relu(x), 2)\n",
    "\n",
    "        x = x.flatten()\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)        \n",
    "        #---end---------------------\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759ee961-4e90-498c-a316-398ec85057f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "We can now create a new model instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "13b45f83-8084-4cd9-8aa1-09ec8b83445f",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = LeNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4535a390-bb2f-4695-a4e2-f7c6253ac0e0",
   "metadata": {},
   "source": [
    "We now load the state dict with the filename `lenet.pt` into the model. These weights are already pretrained and should have a high accuracy when detecting MNIST images. Afterwards, we check if the network is able to detect our stored sample.\n",
    "\n",
    "<span style=\"color:green\">Your Task:</span>\n",
    "* <span style=\"color:green\">Load the state_dict `lenet.pt` from disk and load the state dict into the LeNet instance</span>\n",
    "* <span style=\"color:green\">Calculate the output of the network when feeding in the image</span>\n",
    "    * Load the image from disk (`mnist_sample.pt`) into a tensor \n",
    "    * Note that you need to expand the dimensions of the tensor, since the network expects an input with size $N \\times 1 \\times 28 \\times 28$ but the image is size $ 28 \\times 28$. You can create two dimensions by using a slice with **[None, None, :, :]**\n",
    "    * Check if the image is detected correctly. The output with the highest value corresponds to the estimated class (you can use `torch.argmax`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a0244c61-aea6-4425-92e2-6aa0972423a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 28, 28])\n",
      "tensor(6)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#---to-be-done-by-student---\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    " # load model\n",
    "net.load_state_dict(torch.load('./lenet.pt', weights_only=True))\n",
    "\n",
    "# load MNIST sample\n",
    "mnist_sample = torch.empty(1, 1, 28, 28)\n",
    "mnist_sample[None, None, :, :] = torch.load('./mnist_sample.pt')\n",
    "print(mnist_sample.size())\n",
    "\n",
    "# Print MNIST sample\n",
    "plt.imshow(mnist_sample.reshape(28, 28), cmap='gray', interpolation='none')\n",
    "\n",
    "# Detect class\n",
    "out = net(mnist_sample)\n",
    "print(torch.argmax(out))\n",
    "#---end---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84fa4c3-0e1a-4264-8fd2-f550ebed730b",
   "metadata": {},
   "source": [
    "Next, we want to determine the accuracy of the network using the full MNIST test data. Additionally, we want to measure the execution time for the network on the CPU as well as on the GPU.\n",
    "\n",
    "* We first load the complete MNIST testset (10.000 Images), and zero-center and scale it.\n",
    "* We create a DataLoader, which can be iterated with enumerate and returns the data in chunks of 64, so-called batches. The resulting tensor is of size $64 \\times 1 \\times 28 \\times 28$.\n",
    "* The target tensor is of size $64$ where for each image the tensor entry is the correct label number (e.g. image shows a `inputs[8, :, :, :]` shows a two, the corresponding value in the target tensor `targets[8]` is 2.\n",
    "\n",
    "<span style=\"color:green\">Your Task:</span>\n",
    "* <span style=\"color:green\">For every batch load the data into the network.</span>\n",
    "* <span style=\"color:green\">Calculate the overall accuracy (ratio of correctly deteced images to all images).</span>\n",
    "* <span style=\"color:green\">Calculate the overall execution time (forward pass) of the network on the cpu as well as on the gpu.</span>\n",
    "    * <span style=\"color:green\">For GPU calculations you have to load the network as well as the input to the GPU and bring the result back to the CPU for your accuracy calculations.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d1db2c7e-24c9-4511-9557-cccbb208a495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test images: 10000\n",
      "Number of batches: 157\n",
      "Batch shape: torch.Size([64, 1, 28, 28])\n",
      "Target (Labels): tensor([7, 2, 1, 0, 4, 1, 4, 9, 5, 9, 0, 6, 9, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import time\n",
    "\n",
    "test_data = torchvision.datasets.MNIST('.', train=False, download=True, transform=torchvision.transforms.Compose([\n",
    "                                                torchvision.transforms.ToTensor(), torchvision.transforms.Normalize(\n",
    "                                                (0.1307, ), (0.3081)) ]))\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "print(f\"Number of test images: {len(test_data)}\")\n",
    "print(f\"Number of batches: {len(test_loader)}\")\n",
    "_, (inputs, targets) = next(enumerate(test_loader))\n",
    "print(f\"Batch shape: {inputs.size()}\")\n",
    "print(f\"Target (Labels): {targets[0:15]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5e157640-fa0e-4529-9928-1b6b70c42c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LenNet Accuracy is: 97.43%\n",
      "Total time for forward pass: 24.3427s\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "correct_detected = 0\n",
    "accuracy = 0\n",
    "total_time = 0.0\n",
    "\n",
    "net.to(device)\n",
    "net.eval()\n",
    "\n",
    "#---to-be-done-by-student---\n",
    "start_time = time.time()\n",
    "#---end---------------------\n",
    "\n",
    "for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "    #---to-be-done-by-student---\n",
    "    inputs = inputs.to(device)\n",
    "    targets = targets.to(device)\n",
    "    \n",
    "    for idx, mnist_img in enumerate(inputs):\n",
    "        mnist_sample = mnist_img[None, :, :, :]\n",
    "        \n",
    "        if torch.argmax(net(mnist_sample)) == targets[idx]:\n",
    "            correct_detected += 1\n",
    "    #---end---------------------      \n",
    "\n",
    "\n",
    "#---to-be-done-by-student---\n",
    "total_time = time.time() - start_time\n",
    "#---end---------------------\n",
    "\n",
    "accuracy = correct_detected/len(test_data)\n",
    "\n",
    "print(f'LenNet Accuracy is: {accuracy:.2%}')\n",
    "print(f'Total time for forward pass: {round(total_time, 4)}s')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
